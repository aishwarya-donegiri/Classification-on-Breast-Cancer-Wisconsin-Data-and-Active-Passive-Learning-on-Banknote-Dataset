{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name : Aishwarya Donegiri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USC ID : 4640782493"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised, Semi-supervised and Unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Breast Cancer Wisconsin (Diagnostic) Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Class',\n",
       " 'mean radius',\n",
       " 'mean texture',\n",
       " 'mean perimeter',\n",
       " 'mean area',\n",
       " 'mean smoothness',\n",
       " 'mean compactness',\n",
       " 'mean concavity',\n",
       " 'mean concave points',\n",
       " 'mean symmetry',\n",
       " 'mean fractal dimension',\n",
       " 'radius SE',\n",
       " 'texture SE',\n",
       " 'perimeter SE',\n",
       " 'area SE',\n",
       " 'smoothness SE',\n",
       " 'compactness SE',\n",
       " 'concavity SE',\n",
       " 'concave points SE',\n",
       " 'symmetry SE',\n",
       " 'fractal dimension SE',\n",
       " 'worst radius',\n",
       " 'worst texture',\n",
       " 'worst perimeter',\n",
       " 'worst area',\n",
       " 'worst smoothness',\n",
       " 'worst compactness',\n",
       " 'worst concavity',\n",
       " 'worst concave points',\n",
       " 'worst symmetry',\n",
       " 'worst fractal dimension']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_valued_features=[\"radius\", \"texture\", \"perimeter\", 'area', \"smoothness\", \"compactness\", \"concavity\", \"concave points\",\n",
    "                      \"symmetry\", \"fractal dimension\" ]\n",
    "l1=[]\n",
    "l2=[]\n",
    "l3=[]\n",
    "for feature in real_valued_features:\n",
    "    l1.append(\"mean \"+feature)\n",
    "    l2.append(feature+\" SE\")\n",
    "    l3.append(\"worst \"+feature)\n",
    "columns=[\"ID\", \"Class\"]+l1+l2+l3\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID Class  mean radius  mean texture  mean perimeter  mean area  \\\n",
       "0      842302     M        17.99         10.38          122.80     1001.0   \n",
       "1      842517     M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903     M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301     M        11.42         20.38           77.58      386.1   \n",
       "4    84358402     M        20.29         14.34          135.10     1297.0   \n",
       "..        ...   ...          ...           ...             ...        ...   \n",
       "564    926424     M        21.56         22.39          142.00     1479.0   \n",
       "565    926682     M        20.13         28.25          131.20     1261.0   \n",
       "566    926954     M        16.60         28.08          108.30      858.1   \n",
       "567    927241     M        20.60         29.33          140.10     1265.0   \n",
       "568     92751     B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  worst radius  worst texture  worst perimeter  worst area  \\\n",
       "0    ...        25.380          17.33           184.60      2019.0   \n",
       "1    ...        24.990          23.41           158.80      1956.0   \n",
       "2    ...        23.570          25.53           152.50      1709.0   \n",
       "3    ...        14.910          26.50            98.87       567.7   \n",
       "4    ...        22.540          16.67           152.20      1575.0   \n",
       "..   ...           ...            ...              ...         ...   \n",
       "564  ...        25.450          26.40           166.10      2027.0   \n",
       "565  ...        23.690          38.25           155.00      1731.0   \n",
       "566  ...        18.980          34.12           126.70      1124.0   \n",
       "567  ...        25.740          39.42           184.60      1821.0   \n",
       "568  ...         9.456          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.read_csv(\"../data/wdbc.csv\",names=columns)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class  mean radius  mean texture  mean perimeter  mean area  \\\n",
       "0        0        17.99         10.38          122.80     1001.0   \n",
       "1        0        20.57         17.77          132.90     1326.0   \n",
       "2        0        19.69         21.25          130.00     1203.0   \n",
       "3        0        11.42         20.38           77.58      386.1   \n",
       "4        0        20.29         14.34          135.10     1297.0   \n",
       "..     ...          ...           ...             ...        ...   \n",
       "564      0        21.56         22.39          142.00     1479.0   \n",
       "565      0        20.13         28.25          131.20     1261.0   \n",
       "566      0        16.60         28.08          108.30      858.1   \n",
       "567      0        20.60         29.33          140.10     1265.0   \n",
       "568      1         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     mean symmetry  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Class'].replace(to_replace={'M':0,'B':1},inplace=True)\n",
    "dataset.drop('ID',axis=1,inplace=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.06664</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>...</td>\n",
       "      <td>15.110</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.23900</td>\n",
       "      <td>0.12880</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.04568</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>...</td>\n",
       "      <td>14.500</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.02956</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>...</td>\n",
       "      <td>10.230</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.13240</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>13.030</td>\n",
       "      <td>18.42</td>\n",
       "      <td>82.61</td>\n",
       "      <td>523.8</td>\n",
       "      <td>0.08983</td>\n",
       "      <td>0.03766</td>\n",
       "      <td>0.02562</td>\n",
       "      <td>0.029230</td>\n",
       "      <td>0.1467</td>\n",
       "      <td>...</td>\n",
       "      <td>13.300</td>\n",
       "      <td>22.81</td>\n",
       "      <td>84.46</td>\n",
       "      <td>545.9</td>\n",
       "      <td>0.09701</td>\n",
       "      <td>0.04619</td>\n",
       "      <td>0.04833</td>\n",
       "      <td>0.05013</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>0.06169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>8.196</td>\n",
       "      <td>16.84</td>\n",
       "      <td>51.71</td>\n",
       "      <td>201.9</td>\n",
       "      <td>0.08600</td>\n",
       "      <td>0.05943</td>\n",
       "      <td>0.01588</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>...</td>\n",
       "      <td>8.964</td>\n",
       "      <td>21.96</td>\n",
       "      <td>57.26</td>\n",
       "      <td>242.2</td>\n",
       "      <td>0.12970</td>\n",
       "      <td>0.13570</td>\n",
       "      <td>0.06880</td>\n",
       "      <td>0.02564</td>\n",
       "      <td>0.3105</td>\n",
       "      <td>0.07409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1</td>\n",
       "      <td>14.590</td>\n",
       "      <td>22.68</td>\n",
       "      <td>96.39</td>\n",
       "      <td>657.1</td>\n",
       "      <td>0.08473</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.10290</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>...</td>\n",
       "      <td>15.480</td>\n",
       "      <td>27.27</td>\n",
       "      <td>105.90</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.31710</td>\n",
       "      <td>0.36620</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.08004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1</td>\n",
       "      <td>11.510</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.11120</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>...</td>\n",
       "      <td>12.480</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.08732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>1</td>\n",
       "      <td>14.050</td>\n",
       "      <td>27.15</td>\n",
       "      <td>91.38</td>\n",
       "      <td>600.4</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.04462</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>...</td>\n",
       "      <td>15.300</td>\n",
       "      <td>33.17</td>\n",
       "      <td>100.20</td>\n",
       "      <td>706.7</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.08321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>1</td>\n",
       "      <td>11.200</td>\n",
       "      <td>29.37</td>\n",
       "      <td>70.67</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.07449</td>\n",
       "      <td>0.03558</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>...</td>\n",
       "      <td>11.920</td>\n",
       "      <td>38.30</td>\n",
       "      <td>75.19</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.09267</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1</td>\n",
       "      <td>7.760</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>357 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class  mean radius  mean texture  mean perimeter  mean area  \\\n",
       "19       1       13.540         14.36           87.46      566.3   \n",
       "20       1       13.080         15.71           85.63      520.0   \n",
       "21       1        9.504         12.44           60.34      273.9   \n",
       "37       1       13.030         18.42           82.61      523.8   \n",
       "46       1        8.196         16.84           51.71      201.9   \n",
       "..     ...          ...           ...             ...        ...   \n",
       "558      1       14.590         22.68           96.39      657.1   \n",
       "559      1       11.510         23.93           74.52      403.5   \n",
       "560      1       14.050         27.15           91.38      600.4   \n",
       "561      1       11.200         29.37           70.67      386.0   \n",
       "568      1        7.760         24.54           47.92      181.0   \n",
       "\n",
       "     mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "19           0.09779           0.08129         0.06664             0.047810   \n",
       "20           0.10750           0.12700         0.04568             0.031100   \n",
       "21           0.10240           0.06492         0.02956             0.020760   \n",
       "37           0.08983           0.03766         0.02562             0.029230   \n",
       "46           0.08600           0.05943         0.01588             0.005917   \n",
       "..               ...               ...             ...                  ...   \n",
       "558          0.08473           0.13300         0.10290             0.037360   \n",
       "559          0.09261           0.10210         0.11120             0.041050   \n",
       "560          0.09929           0.11260         0.04462             0.043040   \n",
       "561          0.07449           0.03558         0.00000             0.000000   \n",
       "568          0.05263           0.04362         0.00000             0.000000   \n",
       "\n",
       "     mean symmetry  ...  worst radius  worst texture  worst perimeter  \\\n",
       "19          0.1885  ...        15.110          19.26            99.70   \n",
       "20          0.1967  ...        14.500          20.49            96.09   \n",
       "21          0.1815  ...        10.230          15.66            65.13   \n",
       "37          0.1467  ...        13.300          22.81            84.46   \n",
       "46          0.1769  ...         8.964          21.96            57.26   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "558         0.1454  ...        15.480          27.27           105.90   \n",
       "559         0.1388  ...        12.480          37.16            82.28   \n",
       "560         0.1537  ...        15.300          33.17           100.20   \n",
       "561         0.1060  ...        11.920          38.30            75.19   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "19        711.2           0.14400            0.17730          0.23900   \n",
       "20        630.5           0.13120            0.27760          0.18900   \n",
       "21        314.9           0.13240            0.11480          0.08867   \n",
       "37        545.9           0.09701            0.04619          0.04833   \n",
       "46        242.2           0.12970            0.13570          0.06880   \n",
       "..          ...               ...                ...              ...   \n",
       "558       733.5           0.10260            0.31710          0.36620   \n",
       "559       474.2           0.12980            0.25170          0.36300   \n",
       "560       706.7           0.12410            0.22640          0.13260   \n",
       "561       439.6           0.09267            0.05494          0.00000   \n",
       "568       268.6           0.08996            0.06444          0.00000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  \n",
       "19                0.12880          0.2977                  0.07259  \n",
       "20                0.07283          0.3184                  0.08183  \n",
       "21                0.06227          0.2450                  0.07773  \n",
       "37                0.05013          0.1987                  0.06169  \n",
       "46                0.02564          0.3105                  0.07409  \n",
       "..                    ...             ...                      ...  \n",
       "558               0.11050          0.2258                  0.08004  \n",
       "559               0.09653          0.2112                  0.08732  \n",
       "560               0.10480          0.2250                  0.08321  \n",
       "561               0.00000          0.1566                  0.05905  \n",
       "568               0.00000          0.2871                  0.07039  \n",
       "\n",
       "[357 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_class=dataset[dataset['Class']==1]\n",
    "positive_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_positive,X_test_positive,y_train_positive,y_test_positive=train_test_split(positive_class.iloc[:,1:]\n",
    "#                                                                                    ,positive_class.iloc[:,0],\n",
    "#                                                                                    test_size=0.2,shuffle=False)\n",
    "# X_test_positive\n",
    "no_positive=int(len(positive_class)*0.2)\n",
    "positive_train=positive_class.iloc[no_positive+1:,:]\n",
    "positive_test=positive_class.iloc[:no_positive+1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0</td>\n",
       "      <td>20.92</td>\n",
       "      <td>25.09</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.22360</td>\n",
       "      <td>0.31740</td>\n",
       "      <td>0.14740</td>\n",
       "      <td>0.2149</td>\n",
       "      <td>...</td>\n",
       "      <td>24.29</td>\n",
       "      <td>29.41</td>\n",
       "      <td>179.10</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.1407</td>\n",
       "      <td>0.4186</td>\n",
       "      <td>0.6599</td>\n",
       "      <td>0.2542</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.09873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.45</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.2113</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.69</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.1166</td>\n",
       "      <td>0.1922</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.98</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.1139</td>\n",
       "      <td>0.3094</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.74</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.8681</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class  mean radius  mean texture  mean perimeter  mean area  \\\n",
       "0        0        17.99         10.38          122.80     1001.0   \n",
       "1        0        20.57         17.77          132.90     1326.0   \n",
       "2        0        19.69         21.25          130.00     1203.0   \n",
       "3        0        11.42         20.38           77.58      386.1   \n",
       "4        0        20.29         14.34          135.10     1297.0   \n",
       "..     ...          ...           ...             ...        ...   \n",
       "563      0        20.92         25.09          143.00     1347.0   \n",
       "564      0        21.56         22.39          142.00     1479.0   \n",
       "565      0        20.13         28.25          131.20     1261.0   \n",
       "566      0        16.60         28.08          108.30      858.1   \n",
       "567      0        20.60         29.33          140.10     1265.0   \n",
       "\n",
       "     mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "563          0.10990           0.22360         0.31740              0.14740   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "\n",
       "     mean symmetry  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0           0.2419  ...         25.38          17.33           184.60   \n",
       "1           0.1812  ...         24.99          23.41           158.80   \n",
       "2           0.2069  ...         23.57          25.53           152.50   \n",
       "3           0.2597  ...         14.91          26.50            98.87   \n",
       "4           0.1809  ...         22.54          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "563         0.2149  ...         24.29          29.41           179.10   \n",
       "564         0.1726  ...         25.45          26.40           166.10   \n",
       "565         0.1752  ...         23.69          38.25           155.00   \n",
       "566         0.1590  ...         18.98          34.12           126.70   \n",
       "567         0.2397  ...         25.74          39.42           184.60   \n",
       "\n",
       "     worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0        2019.0            0.1622             0.6656           0.7119   \n",
       "1        1956.0            0.1238             0.1866           0.2416   \n",
       "2        1709.0            0.1444             0.4245           0.4504   \n",
       "3         567.7            0.2098             0.8663           0.6869   \n",
       "4        1575.0            0.1374             0.2050           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "563      1819.0            0.1407             0.4186           0.6599   \n",
       "564      2027.0            0.1410             0.2113           0.4107   \n",
       "565      1731.0            0.1166             0.1922           0.3215   \n",
       "566      1124.0            0.1139             0.3094           0.3403   \n",
       "567      1821.0            0.1650             0.8681           0.9387   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "563                0.2542          0.2929                  0.09873  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "\n",
       "[212 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_class=dataset[dataset['Class']==0]\n",
    "negative_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_negative=int(len(negative_class)*0.2)\n",
    "negative_train=negative_class.iloc[no_negative+1:,:]\n",
    "negative_test=negative_class.iloc[:no_negative+1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1</td>\n",
       "      <td>12.25</td>\n",
       "      <td>17.94</td>\n",
       "      <td>78.27</td>\n",
       "      <td>460.3</td>\n",
       "      <td>0.08654</td>\n",
       "      <td>0.06679</td>\n",
       "      <td>0.03885</td>\n",
       "      <td>0.023310</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>...</td>\n",
       "      <td>13.59</td>\n",
       "      <td>25.22</td>\n",
       "      <td>86.60</td>\n",
       "      <td>564.2</td>\n",
       "      <td>0.12170</td>\n",
       "      <td>0.17880</td>\n",
       "      <td>0.19430</td>\n",
       "      <td>0.08211</td>\n",
       "      <td>0.3113</td>\n",
       "      <td>0.08132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1</td>\n",
       "      <td>16.84</td>\n",
       "      <td>19.46</td>\n",
       "      <td>108.40</td>\n",
       "      <td>880.2</td>\n",
       "      <td>0.07445</td>\n",
       "      <td>0.07223</td>\n",
       "      <td>0.05150</td>\n",
       "      <td>0.027710</td>\n",
       "      <td>0.1844</td>\n",
       "      <td>...</td>\n",
       "      <td>18.22</td>\n",
       "      <td>28.07</td>\n",
       "      <td>120.30</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>0.08774</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.18820</td>\n",
       "      <td>0.08436</td>\n",
       "      <td>0.2527</td>\n",
       "      <td>0.05972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1</td>\n",
       "      <td>12.06</td>\n",
       "      <td>12.74</td>\n",
       "      <td>76.84</td>\n",
       "      <td>448.6</td>\n",
       "      <td>0.09311</td>\n",
       "      <td>0.05241</td>\n",
       "      <td>0.01972</td>\n",
       "      <td>0.019630</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>13.14</td>\n",
       "      <td>18.41</td>\n",
       "      <td>84.08</td>\n",
       "      <td>532.8</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.12320</td>\n",
       "      <td>0.08636</td>\n",
       "      <td>0.07025</td>\n",
       "      <td>0.2514</td>\n",
       "      <td>0.07898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1</td>\n",
       "      <td>10.90</td>\n",
       "      <td>12.96</td>\n",
       "      <td>68.69</td>\n",
       "      <td>366.8</td>\n",
       "      <td>0.07515</td>\n",
       "      <td>0.03718</td>\n",
       "      <td>0.00309</td>\n",
       "      <td>0.006588</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>...</td>\n",
       "      <td>12.36</td>\n",
       "      <td>18.20</td>\n",
       "      <td>78.07</td>\n",
       "      <td>470.0</td>\n",
       "      <td>0.11710</td>\n",
       "      <td>0.08294</td>\n",
       "      <td>0.01854</td>\n",
       "      <td>0.03953</td>\n",
       "      <td>0.2738</td>\n",
       "      <td>0.07685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1</td>\n",
       "      <td>11.75</td>\n",
       "      <td>20.18</td>\n",
       "      <td>76.10</td>\n",
       "      <td>419.8</td>\n",
       "      <td>0.10890</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.06843</td>\n",
       "      <td>0.037380</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>...</td>\n",
       "      <td>13.32</td>\n",
       "      <td>26.21</td>\n",
       "      <td>88.91</td>\n",
       "      <td>543.9</td>\n",
       "      <td>0.13580</td>\n",
       "      <td>0.18920</td>\n",
       "      <td>0.19560</td>\n",
       "      <td>0.07909</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>0.07987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0</td>\n",
       "      <td>20.92</td>\n",
       "      <td>25.09</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.22360</td>\n",
       "      <td>0.31740</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.2149</td>\n",
       "      <td>...</td>\n",
       "      <td>24.29</td>\n",
       "      <td>29.41</td>\n",
       "      <td>179.10</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.14070</td>\n",
       "      <td>0.41860</td>\n",
       "      <td>0.65990</td>\n",
       "      <td>0.25420</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.09873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.45</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.41070</td>\n",
       "      <td>0.22160</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.097910</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.69</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.32150</td>\n",
       "      <td>0.16280</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.053020</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.98</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.34030</td>\n",
       "      <td>0.14180</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.74</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.93870</td>\n",
       "      <td>0.26500</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>454 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class  mean radius  mean texture  mean perimeter  mean area  \\\n",
       "155      1        12.25         17.94           78.27      460.3   \n",
       "157      1        16.84         19.46          108.40      880.2   \n",
       "158      1        12.06         12.74           76.84      448.6   \n",
       "159      1        10.90         12.96           68.69      366.8   \n",
       "160      1        11.75         20.18           76.10      419.8   \n",
       "..     ...          ...           ...             ...        ...   \n",
       "563      0        20.92         25.09          143.00     1347.0   \n",
       "564      0        21.56         22.39          142.00     1479.0   \n",
       "565      0        20.13         28.25          131.20     1261.0   \n",
       "566      0        16.60         28.08          108.30      858.1   \n",
       "567      0        20.60         29.33          140.10     1265.0   \n",
       "\n",
       "     mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "155          0.08654           0.06679         0.03885             0.023310   \n",
       "157          0.07445           0.07223         0.05150             0.027710   \n",
       "158          0.09311           0.05241         0.01972             0.019630   \n",
       "159          0.07515           0.03718         0.00309             0.006588   \n",
       "160          0.10890           0.11410         0.06843             0.037380   \n",
       "..               ...               ...             ...                  ...   \n",
       "563          0.10990           0.22360         0.31740             0.147400   \n",
       "564          0.11100           0.11590         0.24390             0.138900   \n",
       "565          0.09780           0.10340         0.14400             0.097910   \n",
       "566          0.08455           0.10230         0.09251             0.053020   \n",
       "567          0.11780           0.27700         0.35140             0.152000   \n",
       "\n",
       "     mean symmetry  ...  worst radius  worst texture  worst perimeter  \\\n",
       "155         0.1970  ...         13.59          25.22            86.60   \n",
       "157         0.1844  ...         18.22          28.07           120.30   \n",
       "158         0.1590  ...         13.14          18.41            84.08   \n",
       "159         0.1442  ...         12.36          18.20            78.07   \n",
       "160         0.1993  ...         13.32          26.21            88.91   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "563         0.2149  ...         24.29          29.41           179.10   \n",
       "564         0.1726  ...         25.45          26.40           166.10   \n",
       "565         0.1752  ...         23.69          38.25           155.00   \n",
       "566         0.1590  ...         18.98          34.12           126.70   \n",
       "567         0.2397  ...         25.74          39.42           184.60   \n",
       "\n",
       "     worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "155       564.2           0.12170            0.17880          0.19430   \n",
       "157      1032.0           0.08774            0.17100          0.18820   \n",
       "158       532.8           0.12750            0.12320          0.08636   \n",
       "159       470.0           0.11710            0.08294          0.01854   \n",
       "160       543.9           0.13580            0.18920          0.19560   \n",
       "..          ...               ...                ...              ...   \n",
       "563      1819.0           0.14070            0.41860          0.65990   \n",
       "564      2027.0           0.14100            0.21130          0.41070   \n",
       "565      1731.0           0.11660            0.19220          0.32150   \n",
       "566      1124.0           0.11390            0.30940          0.34030   \n",
       "567      1821.0           0.16500            0.86810          0.93870   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  \n",
       "155               0.08211          0.3113                  0.08132  \n",
       "157               0.08436          0.2527                  0.05972  \n",
       "158               0.07025          0.2514                  0.07898  \n",
       "159               0.03953          0.2738                  0.07685  \n",
       "160               0.07909          0.3168                  0.07987  \n",
       "..                    ...             ...                      ...  \n",
       "563               0.25420          0.2929                  0.09873  \n",
       "564               0.22160          0.2060                  0.07115  \n",
       "565               0.16280          0.2572                  0.06637  \n",
       "566               0.14180          0.2218                  0.07820  \n",
       "567               0.26500          0.4087                  0.12400  \n",
       "\n",
       "[454 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=pd.concat([positive_train,negative_train])\n",
    "print (\"Train data :\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.06664</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>...</td>\n",
       "      <td>15.110</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.23900</td>\n",
       "      <td>0.12880</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.04568</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>...</td>\n",
       "      <td>14.500</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.02956</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>...</td>\n",
       "      <td>10.230</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.13240</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>13.030</td>\n",
       "      <td>18.42</td>\n",
       "      <td>82.61</td>\n",
       "      <td>523.8</td>\n",
       "      <td>0.08983</td>\n",
       "      <td>0.03766</td>\n",
       "      <td>0.02562</td>\n",
       "      <td>0.029230</td>\n",
       "      <td>0.1467</td>\n",
       "      <td>...</td>\n",
       "      <td>13.300</td>\n",
       "      <td>22.81</td>\n",
       "      <td>84.46</td>\n",
       "      <td>545.9</td>\n",
       "      <td>0.09701</td>\n",
       "      <td>0.04619</td>\n",
       "      <td>0.04833</td>\n",
       "      <td>0.05013</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>0.06169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>8.196</td>\n",
       "      <td>16.84</td>\n",
       "      <td>51.71</td>\n",
       "      <td>201.9</td>\n",
       "      <td>0.08600</td>\n",
       "      <td>0.05943</td>\n",
       "      <td>0.01588</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>...</td>\n",
       "      <td>8.964</td>\n",
       "      <td>21.96</td>\n",
       "      <td>57.26</td>\n",
       "      <td>242.2</td>\n",
       "      <td>0.12970</td>\n",
       "      <td>0.13570</td>\n",
       "      <td>0.06880</td>\n",
       "      <td>0.02564</td>\n",
       "      <td>0.3105</td>\n",
       "      <td>0.07409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>19.070</td>\n",
       "      <td>24.81</td>\n",
       "      <td>128.30</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>0.09081</td>\n",
       "      <td>0.21900</td>\n",
       "      <td>0.21070</td>\n",
       "      <td>0.099610</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>...</td>\n",
       "      <td>24.090</td>\n",
       "      <td>33.17</td>\n",
       "      <td>177.40</td>\n",
       "      <td>1651.0</td>\n",
       "      <td>0.12470</td>\n",
       "      <td>0.74440</td>\n",
       "      <td>0.72420</td>\n",
       "      <td>0.24930</td>\n",
       "      <td>0.4670</td>\n",
       "      <td>0.10380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>13.280</td>\n",
       "      <td>20.28</td>\n",
       "      <td>87.32</td>\n",
       "      <td>545.2</td>\n",
       "      <td>0.10410</td>\n",
       "      <td>0.14360</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.061580</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>...</td>\n",
       "      <td>17.380</td>\n",
       "      <td>28.00</td>\n",
       "      <td>113.10</td>\n",
       "      <td>907.2</td>\n",
       "      <td>0.15300</td>\n",
       "      <td>0.37240</td>\n",
       "      <td>0.36640</td>\n",
       "      <td>0.14920</td>\n",
       "      <td>0.3739</td>\n",
       "      <td>0.10270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>21.81</td>\n",
       "      <td>85.42</td>\n",
       "      <td>531.5</td>\n",
       "      <td>0.09714</td>\n",
       "      <td>0.10470</td>\n",
       "      <td>0.08259</td>\n",
       "      <td>0.052520</td>\n",
       "      <td>0.1746</td>\n",
       "      <td>...</td>\n",
       "      <td>16.230</td>\n",
       "      <td>29.89</td>\n",
       "      <td>105.50</td>\n",
       "      <td>740.7</td>\n",
       "      <td>0.15030</td>\n",
       "      <td>0.39040</td>\n",
       "      <td>0.37280</td>\n",
       "      <td>0.16070</td>\n",
       "      <td>0.3693</td>\n",
       "      <td>0.09618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>18.650</td>\n",
       "      <td>17.60</td>\n",
       "      <td>123.70</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.16860</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.1907</td>\n",
       "      <td>...</td>\n",
       "      <td>22.820</td>\n",
       "      <td>21.32</td>\n",
       "      <td>150.60</td>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.16790</td>\n",
       "      <td>0.50900</td>\n",
       "      <td>0.73450</td>\n",
       "      <td>0.23780</td>\n",
       "      <td>0.3799</td>\n",
       "      <td>0.09185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>18.66</td>\n",
       "      <td>85.98</td>\n",
       "      <td>534.6</td>\n",
       "      <td>0.11580</td>\n",
       "      <td>0.12310</td>\n",
       "      <td>0.12260</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>...</td>\n",
       "      <td>15.670</td>\n",
       "      <td>27.95</td>\n",
       "      <td>102.80</td>\n",
       "      <td>759.4</td>\n",
       "      <td>0.17860</td>\n",
       "      <td>0.41660</td>\n",
       "      <td>0.50060</td>\n",
       "      <td>0.20880</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>0.11790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class  mean radius  mean texture  mean perimeter  mean area  \\\n",
       "19      1       13.540         14.36           87.46      566.3   \n",
       "20      1       13.080         15.71           85.63      520.0   \n",
       "21      1        9.504         12.44           60.34      273.9   \n",
       "37      1       13.030         18.42           82.61      523.8   \n",
       "46      1        8.196         16.84           51.71      201.9   \n",
       "..    ...          ...           ...             ...        ...   \n",
       "42      0       19.070         24.81          128.30     1104.0   \n",
       "43      0       13.280         20.28           87.32      545.2   \n",
       "44      0       13.170         21.81           85.42      531.5   \n",
       "45      0       18.650         17.60          123.70     1076.0   \n",
       "47      0       13.170         18.66           85.98      534.6   \n",
       "\n",
       "    mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "19          0.09779           0.08129         0.06664             0.047810   \n",
       "20          0.10750           0.12700         0.04568             0.031100   \n",
       "21          0.10240           0.06492         0.02956             0.020760   \n",
       "37          0.08983           0.03766         0.02562             0.029230   \n",
       "46          0.08600           0.05943         0.01588             0.005917   \n",
       "..              ...               ...             ...                  ...   \n",
       "42          0.09081           0.21900         0.21070             0.099610   \n",
       "43          0.10410           0.14360         0.09847             0.061580   \n",
       "44          0.09714           0.10470         0.08259             0.052520   \n",
       "45          0.10990           0.16860         0.19740             0.100900   \n",
       "47          0.11580           0.12310         0.12260             0.073400   \n",
       "\n",
       "    mean symmetry  ...  worst radius  worst texture  worst perimeter  \\\n",
       "19         0.1885  ...        15.110          19.26            99.70   \n",
       "20         0.1967  ...        14.500          20.49            96.09   \n",
       "21         0.1815  ...        10.230          15.66            65.13   \n",
       "37         0.1467  ...        13.300          22.81            84.46   \n",
       "46         0.1769  ...         8.964          21.96            57.26   \n",
       "..            ...  ...           ...            ...              ...   \n",
       "42         0.2310  ...        24.090          33.17           177.40   \n",
       "43         0.1974  ...        17.380          28.00           113.10   \n",
       "44         0.1746  ...        16.230          29.89           105.50   \n",
       "45         0.1907  ...        22.820          21.32           150.60   \n",
       "47         0.2128  ...        15.670          27.95           102.80   \n",
       "\n",
       "    worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "19       711.2           0.14400            0.17730          0.23900   \n",
       "20       630.5           0.13120            0.27760          0.18900   \n",
       "21       314.9           0.13240            0.11480          0.08867   \n",
       "37       545.9           0.09701            0.04619          0.04833   \n",
       "46       242.2           0.12970            0.13570          0.06880   \n",
       "..         ...               ...                ...              ...   \n",
       "42      1651.0           0.12470            0.74440          0.72420   \n",
       "43       907.2           0.15300            0.37240          0.36640   \n",
       "44       740.7           0.15030            0.39040          0.37280   \n",
       "45      1567.0           0.16790            0.50900          0.73450   \n",
       "47       759.4           0.17860            0.41660          0.50060   \n",
       "\n",
       "    worst concave points  worst symmetry  worst fractal dimension  \n",
       "19               0.12880          0.2977                  0.07259  \n",
       "20               0.07283          0.3184                  0.08183  \n",
       "21               0.06227          0.2450                  0.07773  \n",
       "37               0.05013          0.1987                  0.06169  \n",
       "46               0.02564          0.3105                  0.07409  \n",
       "..                   ...             ...                      ...  \n",
       "42               0.24930          0.4670                  0.10380  \n",
       "43               0.14920          0.3739                  0.10270  \n",
       "44               0.16070          0.3693                  0.09618  \n",
       "45               0.23780          0.3799                  0.09185  \n",
       "47               0.20880          0.3900                  0.11790  \n",
       "\n",
       "[115 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.concat([positive_test,negative_test])\n",
    "print (\"Test data :\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (b) Monte Carlo Simulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1-penalised SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc,precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': array([1.00000000e-06, 2.97635144e-06, 8.85866790e-06, 2.63665090e-05,\n",
       "        7.84759970e-05, 2.33572147e-04, 6.95192796e-04, 2.06913808e-03,\n",
       "        6.15848211e-03, 1.83298071e-02, 5.45559478e-02, 1.62377674e-01,\n",
       "        4.83293024e-01, 1.43844989e+00, 4.28133240e+00, 1.27427499e+01,\n",
       "        3.79269019e+01, 1.12883789e+02, 3.35981829e+02, 1.00000000e+03])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_range=np.logspace(-6,3,20)\n",
    "param_grid=dict(C=C_range)\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M :  1\n",
      "Best C :  0.4832930238571752\n",
      "\n",
      "M :  2\n",
      "Best C :  12.742749857031322\n",
      "\n",
      "M :  3\n",
      "Best C :  0.4832930238571752\n",
      "\n",
      "M :  4\n",
      "Best C :  4.281332398719387\n",
      "\n",
      "M :  5\n",
      "Best C :  0.4832930238571752\n",
      "\n",
      "M :  6\n",
      "Best C :  0.4832930238571752\n",
      "\n",
      "M :  7\n",
      "Best C :  4.281332398719387\n",
      "\n",
      "M :  8\n",
      "Best C :  1.438449888287663\n",
      "\n",
      "M :  9\n",
      "Best C :  335.9818286283774\n",
      "\n",
      "M :  10\n",
      "Best C :  1.438449888287663\n",
      "\n",
      "M :  11\n",
      "Best C :  0.4832930238571752\n",
      "\n",
      "M :  12\n",
      "Best C :  1.438449888287663\n",
      "\n",
      "M :  13\n",
      "Best C :  1.438449888287663\n",
      "\n",
      "M :  14\n",
      "Best C :  0.4832930238571752\n",
      "\n",
      "M :  15\n",
      "Best C :  1.438449888287663\n",
      "\n",
      "M :  16\n",
      "Best C :  0.4832930238571752\n",
      "\n",
      "M :  17\n",
      "Best C :  4.281332398719387\n",
      "\n",
      "M :  18\n",
      "Best C :  4.281332398719387\n",
      "\n",
      "M :  19\n",
      "Best C :  4.281332398719387\n",
      "\n",
      "M :  20\n",
      "Best C :  4.281332398719387\n",
      "\n",
      "M :  21\n",
      "Best C :  1.438449888287663\n",
      "\n",
      "M :  22\n",
      "Best C :  1.438449888287663\n",
      "\n",
      "M :  23\n",
      "Best C :  1.438449888287663\n",
      "\n",
      "M :  24\n",
      "Best C :  1.438449888287663\n",
      "\n",
      "M :  25\n",
      "Best C :  1.438449888287663\n",
      "\n",
      "M :  26\n",
      "Best C :  1.438449888287663\n",
      "\n",
      "M :  27\n",
      "Best C :  4.281332398719387\n",
      "\n",
      "M :  28\n",
      "Best C :  1.438449888287663\n",
      "\n",
      "M :  29\n",
      "Best C :  1.438449888287663\n",
      "\n",
      "M :  30\n",
      "Best C :  0.4832930238571752\n",
      "Train data :\n",
      "\n",
      "Average accuracy :  0.9845080763582968\n",
      "Average precision :  0.981675202539104\n",
      "Average F1-score :  0.987747773760072\n",
      "Average Recall :  0.9939181286549706\n",
      "Average AUC :  0.9812785909547045\n",
      "\n",
      "Test data :\n",
      "\n",
      "Average accuracy :  0.9663768115942031\n",
      "Average precision :  0.9658661605810331\n",
      "Average F1-score :  0.9734192972145811\n",
      "Average Recall :  0.9814814814814814\n",
      "Average AUC :  0.9612833763996557\n"
     ]
    }
   ],
   "source": [
    "accuracy_train=[]\n",
    "accuracy_test=[]\n",
    "\n",
    "precision_train=[]\n",
    "precision_test=[]\n",
    "\n",
    "f1_score_train=[]\n",
    "f1_score_test=[]\n",
    "\n",
    "recall_train=[]\n",
    "recall_test=[]\n",
    "\n",
    "auc_train=[]\n",
    "auc_test=[]\n",
    "\n",
    "M=30\n",
    "\n",
    "for m in range(M):\n",
    "    print (\"\\nM : \",m+1)\n",
    "    X_train_positive,X_test_positive,y_train_positive,y_test_positive=train_test_split(positive_class.iloc[:,1:]\n",
    "                                                                                       ,positive_class.iloc[:,0],\n",
    "                                                                                       test_size=0.2)\n",
    "    X_train_negative,X_test_negative,y_train_negative,y_test_negative=train_test_split(negative_class.iloc[:,1:]\n",
    "                                                                                       ,negative_class.iloc[:,0],\n",
    "                                                                                       test_size=0.2)\n",
    "    X_train=pd.concat([X_train_positive,X_train_negative])\n",
    "    y_train=pd.concat([y_train_positive,y_train_negative])\n",
    "    X_test=pd.concat([X_test_positive,X_test_negative])\n",
    "    y_test=pd.concat([y_test_positive,y_test_negative])\n",
    "    scalar=MinMaxScaler()\n",
    "#     scalar=StandardScaler()\n",
    "    X_train=scalar.fit_transform((X_train))\n",
    "    X_test=scalar.transform((X_test))\n",
    "\n",
    "    supervised_model=LinearSVC(penalty='l1',dual=False,max_iter=9000)\n",
    "    grid_supervised=GridSearchCV(estimator=supervised_model,param_grid=param_grid,n_jobs=-1,cv=5)\n",
    "    grid_supervised.fit(X_train,y_train)\n",
    "\n",
    "    y_pred_train=grid_supervised.predict(X_train)\n",
    "    y_pred_test=grid_supervised.predict(X_test)\n",
    "\n",
    "    print (\"Best C : \",grid_supervised.best_params_['C'])\n",
    "\n",
    "    accuracy_train.append(accuracy_score(y_train,y_pred_train))\n",
    "    precision_train.append(precision_score(y_train,y_pred_train))\n",
    "    recall_train.append(recall_score(y_train,y_pred_train))\n",
    "    f1_score_train.append(f1_score(y_train,y_pred_train))\n",
    "    fpr_train,tpr_train,t=roc_curve(y_train,y_pred_train)\n",
    "    auc_train.append(auc(fpr_train,tpr_train))\n",
    "\n",
    "    accuracy_test.append(accuracy_score(y_test,y_pred_test))\n",
    "    precision_test.append(precision_score(y_test,y_pred_test))\n",
    "    recall_test.append(recall_score(y_test,y_pred_test))\n",
    "    f1_score_test.append(f1_score(y_test,y_pred_test))\n",
    "    fpr_test,tpr_test,t=roc_curve(y_test,y_pred_test)\n",
    "    auc_test.append(auc(fpr_test,tpr_test))\n",
    "    \n",
    "print (\"Train data :\\n\")\n",
    "print (\"Average accuracy : \",np.mean(accuracy_train))\n",
    "print (\"Average precision : \",np.mean(precision_train))\n",
    "print (\"Average F1-score : \",np.mean(f1_score_train))\n",
    "print (\"Average Recall : \",np.mean(recall_train))\n",
    "print (\"Average AUC : \",np.mean(auc_train))\n",
    "\n",
    "print (\"\\nTest data :\\n\")\n",
    "print (\"Average accuracy : \",np.mean(accuracy_test))\n",
    "print (\"Average precision : \",np.mean(precision_test))\n",
    "print (\"Average F1-score : \",np.mean(f1_score_test))\n",
    "print (\"Average Recall : \",np.mean(recall_test))\n",
    "print (\"Average AUC : \",np.mean(auc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for train data : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[161,   8],\n",
       "       [  3, 282]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Confusion matrix for train data : \")\n",
    "confusion_matrix(y_train,y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for test data : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[41,  2],\n",
       "       [ 1, 71]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Confusion matrix for test data : \")\n",
    "confusion_matrix(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC Curve for train data : \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"\\nROC Curve for train data : \\n\")\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='darkorange',lw=2, label='ROC curve (area = %0.2f)' % auc(fpr_train,tpr_train))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC Curve for test data : \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"\\nROC Curve for test data : \\n\")\n",
    "plt.figure()\n",
    "plt.plot(fpr_test, tpr_test, color='darkorange',lw=2, label='ROC curve (area = %0.2f)' % auc(fpr_test,tpr_test))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii. Semi-Supervised Learning/ Self-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M :  1\n",
      "Best C for labeled data :  4.281332398719387\n",
      "Best C :  112.88378916846884\n",
      "M :  2\n",
      "Best C for labeled data :  0.1623776739188721\n",
      "Best C :  0.1623776739188721\n",
      "M :  3\n",
      "Best C for labeled data :  1.438449888287663\n",
      "Best C :  4.281332398719387\n",
      "M :  4\n",
      "Best C for labeled data :  4.281332398719387\n",
      "Best C :  1.438449888287663\n",
      "M :  5\n",
      "Best C for labeled data :  12.742749857031322\n",
      "Best C :  4.281332398719387\n",
      "M :  6\n",
      "Best C for labeled data :  1.438449888287663\n",
      "Best C :  12.742749857031322\n",
      "M :  7\n",
      "Best C for labeled data :  0.4832930238571752\n",
      "Best C :  1.438449888287663\n",
      "M :  8\n",
      "Best C for labeled data :  1.438449888287663\n",
      "Best C :  4.281332398719387\n",
      "M :  9\n",
      "Best C for labeled data :  4.281332398719387\n",
      "Best C :  1.438449888287663\n",
      "M :  10\n",
      "Best C for labeled data :  1.438449888287663\n",
      "Best C :  1.438449888287663\n",
      "M :  11\n",
      "Best C for labeled data :  1.438449888287663\n",
      "Best C :  0.4832930238571752\n",
      "M :  12\n",
      "Best C for labeled data :  4.281332398719387\n",
      "Best C :  1.438449888287663\n",
      "M :  13\n",
      "Best C for labeled data :  1.438449888287663\n",
      "Best C :  1.438449888287663\n",
      "M :  14\n",
      "Best C for labeled data :  0.4832930238571752\n"
     ]
    }
   ],
   "source": [
    "accuracy_train=[]\n",
    "accuracy_test=[]\n",
    "\n",
    "precision_train=[]\n",
    "precision_test=[]\n",
    "\n",
    "f1_score_train=[]\n",
    "f1_score_test=[]\n",
    "\n",
    "recall_train=[]\n",
    "recall_test=[]\n",
    "\n",
    "auc_train=[]\n",
    "auc_test=[]\n",
    "\n",
    "for m in range(M):\n",
    "    \n",
    "    print (\"M : \",m+1)\n",
    "    ## dividing test and train data\n",
    "    X_train_positive,X_test_positive,y_train_positive,y_test_positive=train_test_split(positive_class.iloc[:,1:]\n",
    "                                                                                           ,positive_class.iloc[:,0],\n",
    "                                                                                           test_size=0.2)\n",
    "\n",
    "    X_train_negative,X_test_negative,y_train_negative,y_test_negative=train_test_split(negative_class.iloc[:,1:]\n",
    "                                                                                       ,negative_class.iloc[:,0],\n",
    "                                                                                      test_size=0.2)\n",
    "\n",
    "\n",
    "    ## splitting labeled and unlabeled data for positive and negative classes\n",
    "    X_positive_labeled,X_positive_unlabeled,y_positive_labeled,y_positive_unlabeled=train_test_split(X_train_positive\n",
    "                                                                                           ,y_train_positive,\n",
    "                                                                                           test_size=0.5)\n",
    "    X_negative_labeled,X_negative_unlabeled,y_negative_labeled,y_negative_unlabeled=train_test_split(X_train_negative\n",
    "                                                                                       ,y_train_negative,\n",
    "                                                                                      test_size=0.5)\n",
    "\n",
    "\n",
    "    ## combining the positive and negative class to get labeled and unlabeled data\n",
    "    X_labeled=pd.concat([X_positive_labeled,X_negative_labeled])\n",
    "    y_labeled=pd.concat([y_positive_labeled,y_negative_labeled])\n",
    "    X_unlabeled=pd.concat([X_positive_unlabeled,X_negative_unlabeled])\n",
    "    y_unlabeled=pd.concat([y_positive_unlabeled,y_negative_unlabeled])\n",
    "\n",
    "\n",
    "    ## test data\n",
    "    X_test=pd.concat([X_test_positive,X_test_negative])\n",
    "    y_test=pd.concat([y_test_positive,y_test_negative])\n",
    "\n",
    "    ## Scaling the data\n",
    "    scalar=MinMaxScaler()\n",
    "    X_labeled=scalar.fit_transform(X_labeled)\n",
    "    X_unlabeled=scalar.transform(X_unlabeled) \n",
    "    X_test=scalar.transform(X_test)\n",
    "\n",
    "    ## train model on labeled data\n",
    "    semi_supervised_model=LinearSVC(penalty='l1',dual=False)\n",
    "    grid_semi_supervised=GridSearchCV(estimator=semi_supervised_model,param_grid=param_grid,n_jobs=-1,cv=5)\n",
    "    grid_semi_supervised.fit(X_labeled,y_labeled)\n",
    "    \n",
    "    print (\"Best C for labeled data : \",grid_semi_supervised.best_params_['C'])\n",
    "    \n",
    "    while X_unlabeled.shape!=(0,30):\n",
    "\n",
    "\n",
    "        ## finding the distance of each point from the hyperplane\n",
    "        y=grid_semi_supervised.decision_function(X_unlabeled)\n",
    "        w=np.linalg.norm(grid_semi_supervised.best_estimator_.coef_)\n",
    "        distances=y/w\n",
    "\n",
    "        ## farthest point\n",
    "        X=X_unlabeled[np.argmax(np.abs(distances))]\n",
    "\n",
    "\n",
    "        ## predict the label\n",
    "        y=grid_semi_supervised.predict(X.reshape(1,-1))\n",
    "\n",
    "\n",
    "        ## add X,y to the labeled data\n",
    "        X_labeled=np.append(X_labeled,X.reshape(1,-1),axis=0)\n",
    "        y_labeled=np.append(y_labeled,y,axis=0)\n",
    "\n",
    "\n",
    "        ## remove the row from unlabeled\n",
    "        X_unlabeled=np.delete(X_unlabeled,np.argmax(np.abs(distances)),axis=0)\n",
    "  \n",
    "\n",
    "        ## train model on new train data\n",
    "        semi_supervised_model=LinearSVC(penalty='l1',dual=False)\n",
    "        grid_semi_supervised=GridSearchCV(estimator=semi_supervised_model,param_grid=param_grid,n_jobs=-1,cv=5)\n",
    "        grid_semi_supervised.fit(X_labeled,y_labeled)\n",
    "\n",
    "    ## getting the results\n",
    "    y_pred_train=grid_semi_supervised.predict(X_labeled)\n",
    "   \n",
    "    y_pred_test=grid_semi_supervised.predict(X_test)\n",
    "   \n",
    "\n",
    "    print (\"Best C : \",grid_semi_supervised.best_params_['C'])\n",
    "\n",
    "    \n",
    "    accuracy_train.append(accuracy_score(y_labeled,y_pred_train))\n",
    "    precision_train.append(precision_score(y_labeled,y_pred_train))\n",
    "    recall_train.append(recall_score(y_labeled,y_pred_train))\n",
    "    f1_score_train.append(f1_score(y_labeled,y_pred_train))\n",
    "    fpr_train,tpr_train,t=roc_curve(y_labeled,y_pred_train)\n",
    "    auc_train.append(auc(fpr_train,tpr_train))\n",
    "\n",
    "    \n",
    "    accuracy_test.append(accuracy_score(y_test,y_pred_test))\n",
    "    precision_test.append(precision_score(y_test,y_pred_test))\n",
    "    recall_test.append(recall_score(y_test,y_pred_test))\n",
    "    f1_score_test.append(f1_score(y_test,y_pred_test))\n",
    "    fpr_test,tpr_test,t=roc_curve(y_test,y_pred_test)\n",
    "    auc_test.append(auc(fpr_test,tpr_test))\n",
    "\n",
    "\n",
    "print (\"Train data :\\n\")\n",
    "print (\"Average accuracy : \",np.mean(accuracy_train))\n",
    "print (\"Average precision : \",np.mean(precision_train))\n",
    "print (\"Average F1-score : \",np.mean(f1_score_train))\n",
    "print (\"Average Recall : \",np.mean(recall_train))\n",
    "print (\"Average AUC : \",np.mean(auc_train))\n",
    "\n",
    "print (\"\\nTest data :\\n\")\n",
    "print (\"Average accuracy : \",np.mean(accuracy_test))\n",
    "print (\"Average precision : \",np.mean(precision_test))\n",
    "print (\"Average F1-score : \",np.mean(f1_score_test))\n",
    "print (\"Average Recall : \",np.mean(recall_test))\n",
    "print (\"Average AUC : \",np.mean(auc_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Confusion matrix for train data : \")\n",
    "confusion_matrix(y_labeled,y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Confusion matrix for test data : \")\n",
    "confusion_matrix(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nROC Curve for train data : \\n\")\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='darkorange',lw=2, label='ROC curve (area = %0.2f)' % auc(fpr_train,tpr_train))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nROC Curve for test data : \\n\")\n",
    "plt.figure()\n",
    "plt.plot(fpr_test, tpr_test, color='darkorange',lw=2, label='ROC curve (area = %0.2f)' % auc(fpr_test,tpr_test))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot entirely gaurantee that KMeans will not be stuck in a local minima but we can minimise the chances but running KMeans multiple times and choosing the run which has the minimum error. The 'n_init' parameter in 'sklearn.cluster.KMeans()' initialises the KMeans to run for a certain number of times with different centroid seeds and will choose the run with the least error. Here, I have chosen n_init=100. Also, the 'init' parameter is set to 'random' which chooses n_clusters observations (rows) at random from data for the initial centroids which can avoid the local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train=[]\n",
    "accuracy_test=[]\n",
    "\n",
    "precision_train=[]\n",
    "precision_test=[]\n",
    "\n",
    "f1_score_train=[]\n",
    "f1_score_test=[]\n",
    "\n",
    "recall_train=[]\n",
    "recall_test=[]\n",
    "\n",
    "auc_train=[]\n",
    "auc_test=[]\n",
    "\n",
    "for m in range(M):\n",
    "    \n",
    "    print (\"M : \",m+1)\n",
    "\n",
    "    X_train_positive,X_test_positive,y_train_positive,y_test_positive=train_test_split(positive_class.iloc[:,1:]\n",
    "                                                                                           ,positive_class.iloc[:,0],\n",
    "                                                                                           test_size=0.2)\n",
    "    X_train_negative,X_test_negative,y_train_negative,y_test_negative=train_test_split(negative_class.iloc[:,1:]\n",
    "                                                                                       ,negative_class.iloc[:,0],\n",
    "                                                                                       test_size=0.2)\n",
    "\n",
    "    ## test and train data\n",
    "    scalar=MinMaxScaler()\n",
    "    X_train=pd.DataFrame(scalar.fit_transform(pd.concat([X_train_positive,X_train_negative])),columns=X_train_positive.columns)\n",
    "    y_train=pd.concat([y_train_positive,y_train_negative]).reset_index(drop=True)\n",
    "    X_test=pd.DataFrame(scalar.transform(pd.concat([X_test_positive,X_test_negative])),columns=X_train_positive.columns)\n",
    "    y_test=pd.concat([y_test_positive,y_test_negative]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    ## running KMeans with k=2\n",
    "    k_means=KMeans(n_clusters=2,n_init=100,n_jobs=-1,init='random')\n",
    "    k_means.fit(X_train)\n",
    "    labels=pd.DataFrame({'label':k_means.labels_})\n",
    "    train_data=pd.concat([X_train,y_train],axis=1)\n",
    "    new_dataset=pd.concat([train_data,labels],axis=1)\n",
    "    new_dataset\n",
    "\n",
    "    ## finding the centers of the two clusters\n",
    "    centers=k_means.cluster_centers_\n",
    "\n",
    "\n",
    "    ## finding the closest 30 points to the cluster centers to get the cluster labels\n",
    "    grouped=new_dataset.groupby('label')\n",
    "    y_true_train=[]\n",
    "    y_pred_train=[]\n",
    "    y=dict()\n",
    "    for j in range(2):\n",
    "        group=grouped.get_group(j).reset_index(drop=True)\n",
    "        y_true_train+=list(group['Class'])\n",
    "        distances=pd.DataFrame({'distance':k_means.transform(group.iloc[:,:-2])[:,j]})\n",
    "        temp=pd.concat([group,distances],axis=1)\n",
    "        temp=(temp.sort_values(by=['distance'])).iloc[:30,:-1]\n",
    "        y_pred=temp['Class']. value_counts(). idxmax()\n",
    "        y_pred_train+=[y_pred]*len(group)\n",
    "        y[j]=y_pred\n",
    "\n",
    "    accuracy_train.append(accuracy_score(y_true_train,y_pred_train))\n",
    "    precision_train.append(precision_score(y_true_train,y_pred_train))\n",
    "    recall_train.append(recall_score(y_true_train,y_pred_train))\n",
    "    f1_score_train.append(f1_score(y_true_train,y_pred_train))\n",
    "    fpr_train,tpr_train,t=roc_curve(y_true_train,y_pred_train)\n",
    "    auc_train.append(auc(fpr_train,tpr_train))\n",
    "\n",
    "    ## finding the labels for the test data \n",
    "    y_clusters=k_means.predict(X_test)\n",
    "    y_pred_test=[y[i] for i in y_clusters]\n",
    "\n",
    "\n",
    "    accuracy_test.append(accuracy_score(y_test,y_pred_test))\n",
    "    precision_test.append(precision_score(y_test,y_pred_test))\n",
    "    recall_test.append(recall_score(y_test,y_pred_test))\n",
    "    f1_score_test.append(f1_score(y_test,y_pred_test))\n",
    "    fpr_test,tpr_test,t=roc_curve(y_test,y_pred_test)\n",
    "    auc_test.append(auc(fpr_test,tpr_test))\n",
    "\n",
    "print (\"Train data :\\n\")\n",
    "print (\"Average accuracy : \",np.mean(accuracy_train))\n",
    "print (\"Average precision : \",np.mean(precision_train))\n",
    "print (\"Average F1-score : \",np.mean(f1_score_train))\n",
    "print (\"Average Recall : \",np.mean(recall_train))\n",
    "print (\"Average AUC : \",np.mean(auc_train))\n",
    "\n",
    "print (\"\\nTest data :\\n\")\n",
    "print (\"Average accuracy : \",np.mean(accuracy_test))\n",
    "print (\"Average precision : \",np.mean(precision_test))\n",
    "print (\"Average F1-score : \",np.mean(f1_score_test))\n",
    "print (\"Average Recall : \",np.mean(recall_test))\n",
    "print (\"Average AUC : \",np.mean(auc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Confusion matrix for train data : \")\n",
    "confusion_matrix(y_true_train,y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Confusion matrix for test data : \")\n",
    "confusion_matrix(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nROC Curve for train data : \\n\")\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='darkorange',lw=2, label='ROC curve (area = %0.2f)' % auc(fpr_train,tpr_train))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nROC Curve for test data : \\n\")\n",
    "plt.figure()\n",
    "plt.plot(fpr_test, tpr_test, color='darkorange',lw=2, label='ROC curve (area = %0.2f)' % auc(fpr_test,tpr_test))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iv. Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train=[]\n",
    "accuracy_test=[]\n",
    "\n",
    "precision_train=[]\n",
    "precision_test=[]\n",
    "\n",
    "f1_score_train=[]\n",
    "f1_score_test=[]\n",
    "\n",
    "recall_train=[]\n",
    "recall_test=[]\n",
    "\n",
    "auc_train=[]\n",
    "auc_test=[]\n",
    "\n",
    "for m in range(M):\n",
    "    \n",
    "    print (\"M : \",m+1)\n",
    "\n",
    "    X_train_positive,X_test_positive,y_train_positive,y_test_positive=train_test_split(positive_class.iloc[:,1:]\n",
    "                                                                                           ,positive_class.iloc[:,0],\n",
    "                                                                                           test_size=0.2)\n",
    "    X_train_negative,X_test_negative,y_train_negative,y_test_negative=train_test_split(negative_class.iloc[:,1:]\n",
    "                                                                                       ,negative_class.iloc[:,0],\n",
    "                                                                                       test_size=0.2)\n",
    "\n",
    "    ## test and train data\n",
    "    scalar=MinMaxScaler()\n",
    "    X_train=pd.DataFrame(scalar.fit_transform(pd.concat([X_train_positive,X_train_negative])),columns=X_train_positive.columns)\n",
    "    y_train=pd.concat([y_train_positive,y_train_negative]).reset_index(drop=True)\n",
    "    X_test=pd.DataFrame(scalar.transform(pd.concat([X_test_positive,X_test_negative])),columns=X_train_positive.columns)\n",
    "    y_test=pd.concat([y_test_positive,y_test_negative]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    ## running KMeans with k=2\n",
    "    spectral_clustering=SpectralClustering(n_clusters=2,n_jobs=-1)\n",
    "    labels=pd.DataFrame({'label':spectral_clustering.fit_predict(X_train)})\n",
    "    train_data=pd.concat([X_train,y_train],axis=1)\n",
    "    new_dataset=pd.concat([train_data,labels],axis=1)\n",
    "\n",
    "\n",
    "    ## finding the cluster labels\n",
    "    grouped=new_dataset.groupby('label')\n",
    "    y_true_train=[]\n",
    "    y_pred_train=[]\n",
    "    y=dict()\n",
    "    for j in range(2):\n",
    "        group=grouped.get_group(j).reset_index(drop=True)\n",
    "        y_true_train+=list(group['Class'])\n",
    "        y_pred=group['Class']. value_counts(). idxmax()\n",
    "        y_pred_train+=[y_pred]*len(group)\n",
    "        y[j]=y_pred\n",
    "        \n",
    "    accuracy_train.append(accuracy_score(y_true_train,y_pred_train))\n",
    "    precision_train.append(precision_score(y_true_train,y_pred_train))\n",
    "    recall_train.append(recall_score(y_true_train,y_pred_train))\n",
    "    f1_score_train.append(f1_score(y_true_train,y_pred_train))\n",
    "    fpr_train,tpr_train,t=roc_curve(y_true_train,y_pred_train)\n",
    "    auc_train.append(auc(fpr_train,tpr_train))\n",
    "\n",
    "    ## finding the labels for the test data \n",
    "    y_clusters=spectral_clustering.fit_predict(X_test)\n",
    "    y_pred_test=[y[i] for i in y_clusters]\n",
    "\n",
    "    \n",
    "    accuracy_test.append(accuracy_score(y_test,y_pred_test))\n",
    "    precision_test.append(precision_score(y_test,y_pred_test))\n",
    "    recall_test.append(recall_score(y_test,y_pred_test))\n",
    "    f1_score_test.append(f1_score(y_test,y_pred_test))\n",
    "    fpr_test,tpr_test,t=roc_curve(y_test,y_pred_test)\n",
    "    auc_test.append(auc(fpr_test,tpr_test))\n",
    "\n",
    "print (\"Train data :\\n\")\n",
    "print (\"Average accuracy : \",np.mean(accuracy_train))\n",
    "print (\"Average precision : \",np.mean(precision_train))\n",
    "print (\"Average F1-score : \",np.mean(f1_score_train))\n",
    "print (\"Average Recall : \",np.mean(recall_train))\n",
    "print (\"Average AUC : \",np.mean(auc_train))\n",
    "\n",
    "print (\"\\nTest data :\\n\")\n",
    "print (\"Average accuracy : \",np.mean(accuracy_test))\n",
    "print (\"Average precision : \",np.mean(precision_test))\n",
    "print (\"Average F1-score : \",np.mean(f1_score_test))\n",
    "print (\"Average Recall : \",np.mean(recall_test))\n",
    "print (\"Average AUC : \",np.mean(auc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Confusion matrix for train data : \")\n",
    "confusion_matrix(y_true_train,y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Confusion matrix for test data : \")\n",
    "confusion_matrix(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nROC Curve for train data : \\n\")\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='darkorange',lw=2, label='ROC curve (area = %0.2f)' % auc(fpr_train,tpr_train))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nROC Curve for test data : \\n\")\n",
    "plt.figure()\n",
    "plt.plot(fpr_test, tpr_test, color='darkorange',lw=2, label='ROC curve (area = %0.2f)' % auc(fpr_test,tpr_test))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning gives the best value for the average scores (accuracy, precision, recall, F1 score and AUC) followed by Semi-supervised Learning, Unsupervised Learning and Spectral Clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
